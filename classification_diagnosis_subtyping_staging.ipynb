{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c93ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_fscore_support, accuracy_score\n",
    "import torch\n",
    "from timm.utils import AverageMeter,dispatch_clip_grad\n",
    "from timm.models import  model_parameters\n",
    "from collections import OrderedDict\n",
    "from torch.nn.functional import one_hot\n",
    "import random\n",
    "import csv\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.cuda.amp import GradScaler\n",
    "from contextlib import suppress\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from timm.utils import AverageMeter,dispatch_clip_grad\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb99eecb",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3beb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = r'F:\\lung_dl\\data\\subtyping\\TCGA-BRCA R50' #################################\n",
    "model_path = r\"F:\\lung_dl\\code\\Sub-typing\\results\"  # type=str, help='Output path'#############################################################################################################################\n",
    "project = 'topo_mil'  # type=str, help='Project name of exp'#####################################################################################################################\n",
    "seed = 2021  # type=int, help='random number [2021]'\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")  # 例如：2025-06-19_00-40-12\n",
    "title = 'TCGA-BRCA R50'+\"_\"+str(timestamp)+\"seed_\"+str(seed)  # type=str, help='Title of exp'###################################################################################\n",
    "\n",
    "cv_fold = 5  \n",
    "val_ratio = 0.  \n",
    "\n",
    "fold_start = 0  # type=int, help='Start validation fold [0]'\n",
    "amp = False  # action='store_true', help='Automatic Mixed Precision Training'\n",
    "\n",
    "batch_size = 1  # type=int, help='Number of batch size'\n",
    "num_workers = 0  # type=int, help='Number of workers in the dataloader'\n",
    "lr = 2e-4  # type=float, help='Initial learning rate [0.0002]'\n",
    "weight_decay = 1e-5  # type=float, help='Weight decay [5e-3]'\n",
    "num_epoch = 200  # type=int, help='Number of total training epochs [200]'\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.join(model_path,project)):\n",
    "    os.mkdir(os.path.join(model_path,project))\n",
    "model_path = os.path.join(model_path,project,title)\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa26abb",
   "metadata": {},
   "source": [
    "# 数据类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtyping_Dataset(Dataset):\n",
    "    def __init__(self, file_name, file_label):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.patient_name = file_name\n",
    "        self.patient_label = file_label\n",
    "\n",
    "        self.slide_label = [ 0 if _l == 'IDC' else 1 for _l in self.patient_label] # brca################################################################################\n",
    "        #self.slide_label = [ 0 if _l == 'LUAD' else 1 for _l in self.patient_label]# nsclc###################################################################################\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_name)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.slide_label[idx]\n",
    "        file_path = self.patient_name[idx]\n",
    "        features = torch.load(r\"F:\\lung_dl\\data\\subtyping\\TCGA-BRCA R50\\pt_files\"+\"\\\\\"+file_path+\".pt\")#########################################################\n",
    "        return features , int(label)\n",
    "\n",
    "\n",
    "# diagnosis####################################################################################################################################\n",
    "class C16Dataset(Dataset):\n",
    "    def __init__(self, file_name=None, file_label=None,max_patch=-1,root=None,persistence=True,keep_same_psize=0,is_train=False,_type='nsclc'):\n",
    "        super(C16Dataset, self).__init__()\n",
    "        self.file_name = file_name\n",
    "        self.slide_label = file_label\n",
    "        self.slide_label = [int(_l) for _l in self.slide_label]\n",
    "        self.size = len(self.file_name)\n",
    "        self.root = root\n",
    "        self.persistence = persistence\n",
    "        self.keep_same_psize = keep_same_psize\n",
    "        self.is_train = is_train\n",
    "\n",
    "        if persistence:\n",
    "            self.feats = [ torch.load(os.path.join(root,'pt', _f+'.pt')) for _f in file_name ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        :param idx: the index of item\n",
    "        :return: image and its label\n",
    "        \"\"\"\n",
    "        if self.persistence:\n",
    "            features = self.feats[idx]\n",
    "        else:\n",
    "            dir_path = os.path.join(self.root,\"pt\")\n",
    "\n",
    "            file_path = os.path.join(dir_path, self.file_name[idx]+'.pt')\n",
    "            features = torch.load(file_path)\n",
    "\n",
    "        label = int(self.slide_label[idx])\n",
    "\n",
    "\n",
    "        return features , label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19551cb",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "def initialize_weights(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            # ref from huggingface\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m,nn.Linear):\n",
    "            # ref from clam\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m,nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, GlobalAttention\n",
    "\n",
    "class TopoAggregator(nn.Module):\n",
    "    def __init__(self, dim_in=512, dim_hidden=512, topk=6):\n",
    "        super().__init__()\n",
    "        self.proj_q = nn.Linear(dim_in, dim_hidden)\n",
    "        self.proj_k = nn.Linear(dim_in, dim_hidden)\n",
    "        self.topk = topk\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.proj_q(x)  # Query\n",
    "        k = self.proj_k(x) \n",
    "\n",
    "        S = torch.matmul(q, k.transpose(-2, -1))  \n",
    "        S_topk, idx_topk = torch.topk(S, k=self.topk, dim=-1)\n",
    "        idx_topk = idx_topk.to(torch.long)\n",
    "\n",
    "        idx_topk_exp = idx_topk.expand(k.size(0), -1, -1)\n",
    "        batch_indices = torch.arange(k.size(0)).view(-1, 1, 1).to(idx_topk.device)\n",
    "        K_neighbors = k[batch_indices, idx_topk_exp, :]\n",
    "\n",
    "        P_topk = F.softmax(S_topk, dim=2)\n",
    "        X_agg = torch.mul(P_topk.unsqueeze(-1), K_neighbors) + torch.matmul((1 - P_topk).unsqueeze(-1), q.unsqueeze(2))\n",
    "\n",
    "        G = torch.tanh(X_agg)\n",
    "        W_KA = torch.einsum('ijkl,ijkm->ijk', K_neighbors, G)\n",
    "        P_KA = F.softmax(W_KA, dim=2).unsqueeze(2)\n",
    "        X_topo = torch.matmul(P_KA, K_neighbors).squeeze(2)\n",
    "\n",
    "        return X_topo + q\n",
    "\n",
    "\n",
    "class DAttention(nn.Module):\n",
    "    def __init__(self,input_dim,n_classes,TopoAggregator=None):\n",
    "        super(DAttention, self).__init__()\n",
    "        self.L = 512 #512\n",
    "        self.D = 128 #128\n",
    "        self.K = 1\n",
    "        self.feature = [nn.Linear(input_dim, 512)]# nn.LayerNorm(input_dim),\n",
    "        self.feature += [nn.ReLU()]# \n",
    "\n",
    "        #if dropout:\n",
    "        self.feature += [nn.Dropout(0.25)]\n",
    "        if TopoAggregator is not None:\n",
    "            self.feature += [TopoAggregator] \n",
    "        self.feature = nn.Sequential(*self.feature)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            # nn.LayerNorm(self.L),\n",
    "            nn.Linear(self.L, self.D),\n",
    "            # nn.LayerNorm(self.D),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.D, self.K)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.L*self.K, n_classes),\n",
    "        )\n",
    "\n",
    "        self.apply(initialize_weights)\n",
    "    def forward(self, x):\n",
    "\n",
    "        feature = self.feature(x) #1 * N * 512\n",
    "        feature = feature.squeeze(0) # N * 512 \n",
    "        A = self.attention(feature) # N * 1\n",
    "\n",
    "        A = torch.transpose(A, -1, -2)  # 1*N\n",
    "        A = F.softmax(A, dim=-1)  # 1 * N\n",
    "\n",
    "        M = torch.mm(A, feature)  # KxL\n",
    "        Y_prob = self.classifier(M)\n",
    "\n",
    "        return Y_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7867f709",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ff0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=2021):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False   \n",
    "\n",
    "def readCSV(filename):\n",
    "    lines = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        csvreader = csv.reader(f)\n",
    "        for line in csvreader:\n",
    "            lines.append(line)\n",
    "    return lines\n",
    "\n",
    "def get_patient_label(csv_file):\n",
    "    patients_list=[]\n",
    "    labels_list=[]\n",
    "    label_file = readCSV(csv_file)\n",
    "    for i in range(0, len(label_file)):\n",
    "        patients_list.append(label_file[i][0])\n",
    "        labels_list.append(label_file[i][1])\n",
    "    a=Counter(labels_list)\n",
    "    print(\"patient_len:{} label_len:{}\".format(len(patients_list), len(labels_list)))\n",
    "    print(\"all_counter:{}\".format(dict(a)))\n",
    "    return np.array(patients_list,dtype=object), np.array(labels_list,dtype=object)\n",
    "\n",
    "def data_split(full_list, ratio, shuffle=True,label=None,label_balance_val=True):\n",
    "    \"\"\"\n",
    "    dataset split: split the full_list randomly into two sublist (val-set and train-set) based on the ratio\n",
    "    :param full_list: \n",
    "    :param ratio:     \n",
    "    :param shuffle:  \n",
    "    \"\"\"\n",
    "    # select the val-set based on the label ratio\n",
    "    if label_balance_val and label is not None:\n",
    "        _label = label[full_list]\n",
    "        _label_uni = np.unique(_label)\n",
    "        sublist_1 = []\n",
    "        sublist_2 = []\n",
    "\n",
    "        for _l in _label_uni:\n",
    "            _list = full_list[_label == _l]\n",
    "            n_total = len(_list)\n",
    "            offset = int(n_total * ratio)\n",
    "            if shuffle:\n",
    "                random.shuffle(_list)\n",
    "            sublist_1.extend(_list[:offset])\n",
    "            sublist_2.extend(_list[offset:])\n",
    "    else:\n",
    "        n_total = len(full_list)\n",
    "        offset = int(n_total * ratio)\n",
    "        if n_total == 0 or offset < 1:\n",
    "            return [], full_list\n",
    "        if shuffle:\n",
    "            random.shuffle(full_list)\n",
    "        val_set = full_list[:offset]\n",
    "        train_set = full_list[offset:]\n",
    "\n",
    "    return val_set, train_set\n",
    "\n",
    "def get_kflod(k, patients_array, labels_array,val_ratio=False,label_balance_val=True):\n",
    "    if k > 1:\n",
    "        skf = StratifiedKFold(n_splits=k)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    train_patients_list = []\n",
    "    train_labels_list = []\n",
    "    test_patients_list = []\n",
    "    test_labels_list = []\n",
    "    val_patients_list = []\n",
    "    val_labels_list = []\n",
    "    for train_index, test_index in skf.split(patients_array, labels_array):\n",
    "        if val_ratio != 0.:\n",
    "            val_index,train_index = data_split(train_index,val_ratio,True,labels_array,label_balance_val)\n",
    "            x_val, y_val = patients_array[val_index], labels_array[val_index]\n",
    "        else:\n",
    "            x_val, y_val = [],[]\n",
    "        x_train, x_test = patients_array[train_index], patients_array[test_index]\n",
    "        y_train, y_test = labels_array[train_index], labels_array[test_index]\n",
    "\n",
    "        train_patients_list.append(x_train)\n",
    "        train_labels_list.append(y_train)\n",
    "        test_patients_list.append(x_test)\n",
    "        test_labels_list.append(y_test)\n",
    "        val_patients_list.append(x_val)\n",
    "        val_labels_list.append(y_val)\n",
    "        \n",
    "    # print(\"get_kflod.type:{}\".format(type(np.array(train_patients_list))))\n",
    "    return np.array(train_patients_list,dtype=object), np.array(train_labels_list,dtype=object), np.array(test_patients_list,dtype=object), np.array(test_labels_list,dtype=object),np.array(val_patients_list,dtype=object), np.array(val_labels_list,dtype=object)\n",
    "\n",
    "def optimal_thresh(fpr, tpr, thresholds, p=0):\n",
    "    loss = (fpr - tpr) - p * tpr / (fpr + tpr + 1)\n",
    "    idx = np.argmin(loss, axis=0)\n",
    "    return fpr[idx], tpr[idx], thresholds[idx]\n",
    "\n",
    "def five_scores(bag_labels, bag_predictions,sub_typing=False):#multi classification！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "    fpr, tpr, threshold = roc_curve(bag_labels, bag_predictions, pos_label=1)\n",
    "    fpr_optimal, tpr_optimal, threshold_optimal = optimal_thresh(fpr, tpr, threshold)\n",
    "    # threshold_optimal=0.5\n",
    "    auc_value = roc_auc_score(bag_labels, bag_predictions)\n",
    "    this_class_label = np.array(bag_predictions)\n",
    "    this_class_label[this_class_label>=threshold_optimal] = 1\n",
    "    this_class_label[this_class_label<threshold_optimal] = 0\n",
    "    bag_predictions = this_class_label\n",
    "    avg = 'macro' if sub_typing else 'binary'\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(bag_labels, bag_predictions, average=avg)\n",
    "    accuracy = accuracy_score(bag_labels, bag_predictions)\n",
    "    return accuracy, auc_value, precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c193fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model,loader,optimizer,device,amp_autocast,criterion,scheduler,k,epoch):\n",
    "    start = time.time()\n",
    "    train_loss_log = 0.\n",
    "    model.train()\n",
    "    for i, data in enumerate(loader):\n",
    "        # print(i)\n",
    "        optimizer.zero_grad()\n",
    "        if isinstance(data[0],(list,tuple)):\n",
    "            for i in range(len(data[0])):\n",
    "                data[0][i] = data[0][i].to(device)\n",
    "            bag=data[0]\n",
    "            batch_size=data[0][0].size(0)\n",
    "        else:\n",
    "            bag=data[0].to(device)  # b*n*1024\n",
    "            batch_size=bag.size(0)\n",
    "\n",
    "        label=data[1].to(device)\n",
    "        \n",
    "        with amp_autocast():\n",
    "            train_logits = model(bag)\n",
    "            logit_loss = criterion(train_logits.view(batch_size,-1),label)\n",
    "\n",
    "        train_loss = logit_loss \n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_log = train_loss_log + train_loss.item()\n",
    "\n",
    "    end = time.time()\n",
    "    train_loss_log = train_loss_log/len(loader)\n",
    "    scheduler.step()\n",
    "##########################################\n",
    "    model.eval()\n",
    "    loss_cls_meter = AverageMeter()\n",
    "    bag_logit, bag_labels=[], []\n",
    "    # pred= []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(loader):\n",
    "            if len(data[1]) > 1:\n",
    "                bag_labels.extend(data[1].tolist())\n",
    "            else:\n",
    "                bag_labels.append(data[1].item())\n",
    "\n",
    "            if isinstance(data[0],(list,tuple)):\n",
    "                for i in range(len(data[0])):\n",
    "                    data[0][i] = data[0][i].to(device)\n",
    "                bag=data[0]\n",
    "                batch_size=data[0][0].size(0)\n",
    "            else:\n",
    "                bag=data[0].to(device)  # b*n*1024\n",
    "                batch_size=bag.size(0)\n",
    "\n",
    "            label=data[1].to(device)\n",
    "\n",
    "            test_logits = model(bag)\n",
    "\n",
    "            test_loss = criterion(test_logits.view(batch_size,-1),label)\n",
    "            bag_logit.append(torch.softmax(test_logits,dim=-1)[:,1].cpu().squeeze().numpy())\n",
    "\n",
    "\n",
    "            loss_cls_meter.update(test_loss,1)\n",
    "    \n",
    "    accuracy, auc_value, precision, recall, fscore = five_scores(bag_labels, bag_logit, False)#multi classification true############################################################################\n",
    "###################################################\n",
    "\n",
    "    return train_loss_log,start,end,[accuracy, auc_value, precision, recall, fscore]\n",
    "\n",
    "def val_loop(model,loader,device,criterion,epoch):\n",
    "    model.eval()\n",
    "    loss_cls_meter = AverageMeter()\n",
    "    bag_logit, bag_labels=[], []\n",
    "    # pred= []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(loader):\n",
    "            if len(data[1]) > 1:\n",
    "                bag_labels.extend(data[1].tolist())\n",
    "            else:\n",
    "                bag_labels.append(data[1].item())\n",
    "\n",
    "            if isinstance(data[0],(list,tuple)):\n",
    "                for i in range(len(data[0])):\n",
    "                    data[0][i] = data[0][i].to(device)\n",
    "                bag=data[0]\n",
    "                batch_size=data[0][0].size(0)\n",
    "            else:\n",
    "                bag=data[0].to(device)  # b*n*1024\n",
    "                batch_size=bag.size(0)\n",
    "\n",
    "            label=data[1].to(device)\n",
    "\n",
    "            test_logits = model(bag)\n",
    "\n",
    "            test_loss = criterion(test_logits.view(batch_size,-1),label)\n",
    "            bag_logit.append(torch.softmax(test_logits,dim=-1)[:,1].cpu().squeeze().numpy())\n",
    "\n",
    "\n",
    "            loss_cls_meter.update(test_loss,1)\n",
    "    \n",
    "    accuracy, auc_value, precision, recall, fscore = five_scores(bag_labels, bag_logit, False)#multi classification true############################################################################\n",
    "    \n",
    "\n",
    "    return accuracy, auc_value, precision, recall, fscore,loss_cls_meter.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_fold(k,ckc_metric,train_p, train_l, test_p, test_l,val_p,val_l):\n",
    "    # ---> Initialization\n",
    "    seed_torch(seed)\n",
    "    \n",
    "    amp_autocast = torch.cuda.amp.autocast if amp else suppress\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    acs,pre,rec,fs,auc,te_auc,te_fs = ckc_metric\n",
    "\n",
    "    train_set = Subtyping_Dataset(train_p[k],train_l[k])\n",
    "    test_set = Subtyping_Dataset(test_p[k],test_l[k])\n",
    "    if val_ratio != 0.:\n",
    "        val_set = Subtyping_Dataset(val_p[k],val_l[k])\n",
    "    else:\n",
    "        val_set = test_set\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=RandomSampler(train_set), num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # bulid networks\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    age = TopoAggregator(dim_in=512, dim_hidden=512, topk=6).to(device)###############################################################################################################\n",
    "    model = DAttention(input_dim=1024, n_classes=2, TopoAggregator=age).to(device)#######################################################################################PLIP：512，r50为1,\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epoch, 0)\n",
    "\n",
    "    optimal_ac, opt_pre, opt_re, opt_fs, opt_auc,opt_epoch = 0, 0, 0, 0,0,0\n",
    "    epoch_start = 0\n",
    "\n",
    "    import pandas as pd\n",
    "    metrics_df = pd.DataFrame(columns=[\n",
    "        'epoch', \n",
    "        'train_loss', \n",
    "        'val_loss',\n",
    "        'train_accuracy', \n",
    "        'val_accuracy',\n",
    "        'train_auc_value', \n",
    "        'val_auc_value',\n",
    "        'train_precision', \n",
    "        'val_precision', \n",
    "        'train_recall', \n",
    "        'val_recall', \n",
    "        'train_fscore',\n",
    "        'val_fscore'\n",
    "    ])\n",
    "    train_time_meter = AverageMeter()\n",
    "    for epoch in range(epoch_start, num_epoch):\n",
    "        train_loss,start,end,train_metric = train_loop(model,train_loader,optimizer,device,amp_autocast,criterion,scheduler,k,epoch)\n",
    "        train_time_meter.update(end-start)\n",
    "        accuracy, auc_value, precision, recall, fscore, test_loss = val_loop(model,val_loader,device,criterion,epoch)\n",
    "        val_metric=[accuracy, auc_value, precision, recall, fscore]\n",
    "        print('\\r Epoch [%d/%d] train loss: %.1E, test loss: %.1E, accuracy: %.3f, auc_value:%.3f, precision: %.3f, recall: %.3f, fscore: %.3f , time: %.3f(%.3f)' % \n",
    "        (epoch+1, num_epoch, train_loss, test_loss, accuracy, auc_value, precision, recall, fscore, train_time_meter.val,train_time_meter.avg))\n",
    "\n",
    "        new_row = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': test_loss,\n",
    "            'train_accuracy':train_metric[0], \n",
    "            'val_accuracy':val_metric[0],\n",
    "            'train_auc_value':train_metric[1], \n",
    "            'val_auc_value':val_metric[1],\n",
    "            'train_precision':train_metric[2], \n",
    "            'val_precision':val_metric[2], \n",
    "            'train_recall':train_metric[3], \n",
    "            'val_recall':val_metric[3], \n",
    "            'train_fscore':train_metric[4],\n",
    "            'val_fscore':val_metric[4]\n",
    "        }\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        if auc_value > opt_auc:\n",
    "            optimal_ac = accuracy\n",
    "            opt_pre = precision\n",
    "            opt_re = recall\n",
    "            opt_fs = fscore\n",
    "            opt_auc = auc_value\n",
    "            opt_epoch = epoch\n",
    "\n",
    "            if not os.path.exists(model_path):\n",
    "                os.mkdir(model_path)\n",
    "\n",
    "            best_pt = {\n",
    "                'model': model.state_dict(),\n",
    "            }\n",
    "            \n",
    "            torch.save(best_pt, os.path.join(model_path, 'fold_{fold}_epoch_{e}_best_auc_{oa}.pt'.format(fold=k,e=epoch,oa=opt_auc)))\n",
    "\n",
    "\n",
    "    acs.append(optimal_ac)\n",
    "    pre.append(opt_pre)\n",
    "    rec.append(opt_re)\n",
    "    fs.append(opt_fs)\n",
    "    auc.append(opt_auc)\n",
    "    csv_path = os.path.join(model_path, f'metrics_fold_{k}.csv')\n",
    "    metrics_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    return [acs,pre,rec,fs,auc,te_auc,te_fs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93eb9dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch [101/200] train loss: 1.2E-01, test loss: 4.8E-01, accuracy: 0.800, auc_value:0.861, precision: 0.508, recall: 0.775, fscore: 0.614 , time: 337.294(347.998)\n",
      " Epoch [102/200] train loss: 9.0E-02, test loss: 5.5E-01, accuracy: 0.795, auc_value:0.848, precision: 0.500, recall: 0.825, fscore: 0.623 , time: 353.360(348.051)\n",
      " Epoch [103/200] train loss: 1.0E-01, test loss: 5.5E-01, accuracy: 0.795, auc_value:0.845, precision: 0.500, recall: 0.775, fscore: 0.608 , time: 343.011(348.002)\n",
      " Epoch [104/200] train loss: 1.1E-01, test loss: 6.5E-01, accuracy: 0.754, auc_value:0.835, precision: 0.446, recall: 0.825, fscore: 0.579 , time: 345.725(347.980)\n",
      " Epoch [105/200] train loss: 9.7E-02, test loss: 5.5E-01, accuracy: 0.815, auc_value:0.852, precision: 0.534, recall: 0.775, fscore: 0.633 , time: 340.820(347.912)\n",
      " Epoch [106/200] train loss: 1.0E-01, test loss: 5.5E-01, accuracy: 0.774, auc_value:0.836, precision: 0.471, recall: 0.800, fscore: 0.593 , time: 342.346(347.859)\n",
      " Epoch [107/200] train loss: 9.7E-02, test loss: 6.0E-01, accuracy: 0.774, auc_value:0.839, precision: 0.471, recall: 0.800, fscore: 0.593 , time: 336.686(347.755)\n",
      " Epoch [108/200] train loss: 9.9E-02, test loss: 5.6E-01, accuracy: 0.836, auc_value:0.828, precision: 0.583, recall: 0.700, fscore: 0.636 , time: 337.260(347.657)\n",
      " Epoch [109/200] train loss: 9.2E-02, test loss: 5.7E-01, accuracy: 0.795, auc_value:0.860, precision: 0.500, recall: 0.875, fscore: 0.636 , time: 338.015(347.569)\n",
      " Epoch [110/200] train loss: 8.2E-02, test loss: 5.4E-01, accuracy: 0.815, auc_value:0.860, precision: 0.532, recall: 0.825, fscore: 0.647 , time: 336.576(347.469)\n",
      " Epoch [111/200] train loss: 8.5E-02, test loss: 5.8E-01, accuracy: 0.805, auc_value:0.843, precision: 0.517, recall: 0.750, fscore: 0.612 , time: 344.883(347.446)\n",
      " Epoch [112/200] train loss: 7.8E-02, test loss: 6.2E-01, accuracy: 0.744, auc_value:0.839, precision: 0.438, recall: 0.875, fscore: 0.583 , time: 338.492(347.366)\n",
      " Epoch [113/200] train loss: 1.0E-01, test loss: 5.7E-01, accuracy: 0.795, auc_value:0.849, precision: 0.500, recall: 0.800, fscore: 0.615 , time: 339.170(347.293)\n",
      " Epoch [114/200] train loss: 8.3E-02, test loss: 6.0E-01, accuracy: 0.795, auc_value:0.851, precision: 0.500, recall: 0.825, fscore: 0.623 , time: 345.551(347.278)\n",
      " Epoch [115/200] train loss: 1.1E-01, test loss: 6.0E-01, accuracy: 0.785, auc_value:0.842, precision: 0.485, recall: 0.825, fscore: 0.611 , time: 348.165(347.286)\n",
      " Epoch [116/200] train loss: 8.8E-02, test loss: 6.4E-01, accuracy: 0.795, auc_value:0.841, precision: 0.500, recall: 0.800, fscore: 0.615 , time: 343.752(347.255)\n",
      " Epoch [117/200] train loss: 8.4E-02, test loss: 5.6E-01, accuracy: 0.810, auc_value:0.854, precision: 0.525, recall: 0.800, fscore: 0.634 , time: 344.846(347.235)\n",
      " Epoch [118/200] train loss: 7.2E-02, test loss: 6.6E-01, accuracy: 0.779, auc_value:0.836, precision: 0.478, recall: 0.800, fscore: 0.598 , time: 347.922(347.241)\n",
      " Epoch [119/200] train loss: 9.4E-02, test loss: 5.9E-01, accuracy: 0.795, auc_value:0.837, precision: 0.500, recall: 0.800, fscore: 0.615 , time: 363.523(347.377)\n",
      " Epoch [120/200] train loss: 9.2E-02, test loss: 5.7E-01, accuracy: 0.810, auc_value:0.846, precision: 0.525, recall: 0.800, fscore: 0.634 , time: 346.378(347.369)\n",
      " Epoch [121/200] train loss: 6.9E-02, test loss: 6.1E-01, accuracy: 0.749, auc_value:0.835, precision: 0.440, recall: 0.825, fscore: 0.574 , time: 356.740(347.446)\n",
      " Epoch [122/200] train loss: 6.6E-02, test loss: 6.4E-01, accuracy: 0.821, auc_value:0.838, precision: 0.547, recall: 0.725, fscore: 0.624 , time: 358.851(347.540)\n",
      " Epoch [123/200] train loss: 8.7E-02, test loss: 6.4E-01, accuracy: 0.805, auc_value:0.853, precision: 0.516, recall: 0.825, fscore: 0.635 , time: 355.879(347.608)\n",
      " Epoch [124/200] train loss: 7.7E-02, test loss: 6.1E-01, accuracy: 0.790, auc_value:0.838, precision: 0.492, recall: 0.800, fscore: 0.610 , time: 351.381(347.638)\n",
      " Epoch [125/200] train loss: 7.1E-02, test loss: 6.9E-01, accuracy: 0.749, auc_value:0.822, precision: 0.440, recall: 0.825, fscore: 0.574 , time: 346.623(347.630)\n",
      " Epoch [126/200] train loss: 7.8E-02, test loss: 5.9E-01, accuracy: 0.785, auc_value:0.842, precision: 0.486, recall: 0.850, fscore: 0.618 , time: 350.195(347.650)\n",
      " Epoch [127/200] train loss: 7.2E-02, test loss: 6.6E-01, accuracy: 0.785, auc_value:0.822, precision: 0.485, recall: 0.800, fscore: 0.604 , time: 349.545(347.665)\n",
      " Epoch [128/200] train loss: 7.5E-02, test loss: 6.4E-01, accuracy: 0.795, auc_value:0.835, precision: 0.500, recall: 0.800, fscore: 0.615 , time: 347.541(347.664)\n",
      " Epoch [129/200] train loss: 6.3E-02, test loss: 6.7E-01, accuracy: 0.764, auc_value:0.825, precision: 0.458, recall: 0.825, fscore: 0.589 , time: 351.602(347.695)\n",
      " Epoch [130/200] train loss: 8.3E-02, test loss: 6.4E-01, accuracy: 0.785, auc_value:0.835, precision: 0.485, recall: 0.825, fscore: 0.611 , time: 352.665(347.733)\n",
      " Epoch [131/200] train loss: 6.7E-02, test loss: 6.5E-01, accuracy: 0.785, auc_value:0.828, precision: 0.485, recall: 0.800, fscore: 0.604 , time: 351.389(347.761)\n",
      " Epoch [132/200] train loss: 6.5E-02, test loss: 6.7E-01, accuracy: 0.800, auc_value:0.823, precision: 0.508, recall: 0.775, fscore: 0.614 , time: 359.450(347.850)\n",
      " Epoch [133/200] train loss: 6.3E-02, test loss: 6.4E-01, accuracy: 0.790, auc_value:0.832, precision: 0.492, recall: 0.800, fscore: 0.610 , time: 361.871(347.955)\n",
      " Epoch [134/200] train loss: 7.4E-02, test loss: 6.6E-01, accuracy: 0.795, auc_value:0.827, precision: 0.500, recall: 0.800, fscore: 0.615 , time: 348.709(347.961)\n",
      " Epoch [135/200] train loss: 5.4E-02, test loss: 7.3E-01, accuracy: 0.764, auc_value:0.810, precision: 0.457, recall: 0.800, fscore: 0.582 , time: 358.365(348.038)\n",
      " Epoch [136/200] train loss: 5.7E-02, test loss: 7.2E-01, accuracy: 0.790, auc_value:0.816, precision: 0.492, recall: 0.775, fscore: 0.602 , time: 355.384(348.092)\n",
      " Epoch [137/200] train loss: 6.1E-02, test loss: 6.7E-01, accuracy: 0.805, auc_value:0.828, precision: 0.517, recall: 0.750, fscore: 0.612 , time: 342.526(348.051)\n",
      " Epoch [138/200] train loss: 4.8E-02, test loss: 7.8E-01, accuracy: 0.774, auc_value:0.841, precision: 0.472, recall: 0.850, fscore: 0.607 , time: 346.358(348.039)\n",
      " Epoch [139/200] train loss: 6.5E-02, test loss: 7.0E-01, accuracy: 0.795, auc_value:0.830, precision: 0.500, recall: 0.800, fscore: 0.615 , time: 346.124(348.025)\n",
      " Epoch [140/200] train loss: 5.4E-02, test loss: 7.2E-01, accuracy: 0.795, auc_value:0.820, precision: 0.500, recall: 0.775, fscore: 0.608 , time: 346.012(348.011)\n",
      " Epoch [141/200] train loss: 5.2E-02, test loss: 6.9E-01, accuracy: 0.785, auc_value:0.828, precision: 0.485, recall: 0.825, fscore: 0.611 , time: 346.691(348.001)\n",
      " Epoch [142/200] train loss: 5.9E-02, test loss: 7.2E-01, accuracy: 0.795, auc_value:0.825, precision: 0.500, recall: 0.800, fscore: 0.615 , time: 346.275(347.989)\n",
      " Epoch [143/200] train loss: 5.2E-02, test loss: 7.1E-01, accuracy: 0.795, auc_value:0.822, precision: 0.500, recall: 0.775, fscore: 0.608 , time: 346.192(347.977)\n",
      " Epoch [144/200] train loss: 5.1E-02, test loss: 6.8E-01, accuracy: 0.805, auc_value:0.824, precision: 0.517, recall: 0.750, fscore: 0.612 , time: 346.276(347.965)\n",
      " Epoch [145/200] train loss: 4.7E-02, test loss: 6.9E-01, accuracy: 0.779, auc_value:0.827, precision: 0.478, recall: 0.800, fscore: 0.598 , time: 346.335(347.954)\n",
      " Epoch [146/200] train loss: 4.7E-02, test loss: 7.5E-01, accuracy: 0.754, auc_value:0.826, precision: 0.449, recall: 0.875, fscore: 0.593 , time: 346.574(347.944)\n",
      " Epoch [147/200] train loss: 5.1E-02, test loss: 7.2E-01, accuracy: 0.795, auc_value:0.820, precision: 0.500, recall: 0.775, fscore: 0.608 , time: 347.316(347.940)\n",
      " Epoch [148/200] train loss: 4.8E-02, test loss: 6.9E-01, accuracy: 0.800, auc_value:0.825, precision: 0.508, recall: 0.775, fscore: 0.614 , time: 345.916(347.926)\n",
      " Epoch [149/200] train loss: 5.4E-02, test loss: 7.1E-01, accuracy: 0.764, auc_value:0.828, precision: 0.458, recall: 0.825, fscore: 0.589 , time: 348.063(347.927)\n",
      " Epoch [150/200] train loss: 4.7E-02, test loss: 7.3E-01, accuracy: 0.764, auc_value:0.819, precision: 0.457, recall: 0.800, fscore: 0.582 , time: 345.868(347.913)\n",
      " Epoch [151/200] train loss: 4.2E-02, test loss: 7.2E-01, accuracy: 0.749, auc_value:0.827, precision: 0.442, recall: 0.850, fscore: 0.581 , time: 348.131(347.915)\n",
      " Epoch [152/200] train loss: 4.5E-02, test loss: 7.3E-01, accuracy: 0.795, auc_value:0.824, precision: 0.500, recall: 0.775, fscore: 0.608 , time: 346.822(347.908)\n",
      " Epoch [153/200] train loss: 4.7E-02, test loss: 7.5E-01, accuracy: 0.805, auc_value:0.820, precision: 0.517, recall: 0.750, fscore: 0.612 , time: 345.897(347.894)\n",
      " Epoch [154/200] train loss: 4.8E-02, test loss: 7.2E-01, accuracy: 0.759, auc_value:0.820, precision: 0.452, recall: 0.825, fscore: 0.584 , time: 346.116(347.883)\n",
      " Epoch [155/200] train loss: 4.0E-02, test loss: 7.5E-01, accuracy: 0.759, auc_value:0.823, precision: 0.452, recall: 0.825, fscore: 0.584 , time: 347.273(347.879)\n",
      " Epoch [156/200] train loss: 4.4E-02, test loss: 7.4E-01, accuracy: 0.810, auc_value:0.825, precision: 0.527, recall: 0.725, fscore: 0.611 , time: 346.254(347.869)\n",
      " Epoch [157/200] train loss: 4.2E-02, test loss: 7.4E-01, accuracy: 0.759, auc_value:0.826, precision: 0.452, recall: 0.825, fscore: 0.584 , time: 347.629(347.867)\n",
      " Epoch [158/200] train loss: 4.1E-02, test loss: 7.7E-01, accuracy: 0.779, auc_value:0.817, precision: 0.477, recall: 0.775, fscore: 0.590 , time: 348.085(347.868)\n",
      " Epoch [159/200] train loss: 4.0E-02, test loss: 7.8E-01, accuracy: 0.790, auc_value:0.816, precision: 0.492, recall: 0.775, fscore: 0.602 , time: 347.351(347.865)\n",
      " Epoch [160/200] train loss: 4.0E-02, test loss: 7.9E-01, accuracy: 0.810, auc_value:0.816, precision: 0.527, recall: 0.725, fscore: 0.611 , time: 347.320(347.862)\n",
      " Epoch [161/200] train loss: 3.8E-02, test loss: 7.8E-01, accuracy: 0.810, auc_value:0.816, precision: 0.526, recall: 0.750, fscore: 0.619 , time: 346.779(347.855)\n",
      " Epoch [162/200] train loss: 3.2E-02, test loss: 8.1E-01, accuracy: 0.769, auc_value:0.817, precision: 0.464, recall: 0.800, fscore: 0.587 , time: 347.310(347.852)\n",
      " Epoch [163/200] train loss: 3.1E-02, test loss: 8.0E-01, accuracy: 0.805, auc_value:0.816, precision: 0.517, recall: 0.750, fscore: 0.612 , time: 347.644(347.850)\n",
      " Epoch [164/200] train loss: 4.4E-02, test loss: 8.1E-01, accuracy: 0.795, auc_value:0.821, precision: 0.500, recall: 0.750, fscore: 0.600 , time: 347.052(347.846)\n",
      " Epoch [165/200] train loss: 3.9E-02, test loss: 8.0E-01, accuracy: 0.764, auc_value:0.818, precision: 0.457, recall: 0.800, fscore: 0.582 , time: 347.272(347.842)\n",
      " Epoch [166/200] train loss: 3.1E-02, test loss: 8.1E-01, accuracy: 0.764, auc_value:0.817, precision: 0.457, recall: 0.800, fscore: 0.582 , time: 347.556(347.840)\n",
      " Epoch [167/200] train loss: 4.1E-02, test loss: 8.1E-01, accuracy: 0.790, auc_value:0.815, precision: 0.492, recall: 0.750, fscore: 0.594 , time: 347.809(347.840)\n",
      " Epoch [168/200] train loss: 3.2E-02, test loss: 8.3E-01, accuracy: 0.815, auc_value:0.812, precision: 0.537, recall: 0.725, fscore: 0.617 , time: 347.187(347.836)\n",
      " Epoch [169/200] train loss: 3.1E-02, test loss: 8.3E-01, accuracy: 0.795, auc_value:0.812, precision: 0.500, recall: 0.750, fscore: 0.600 , time: 346.404(347.828)\n",
      " Epoch [170/200] train loss: 3.4E-02, test loss: 8.4E-01, accuracy: 0.821, auc_value:0.813, precision: 0.547, recall: 0.725, fscore: 0.624 , time: 347.315(347.825)\n",
      " Epoch [171/200] train loss: 3.0E-02, test loss: 8.3E-01, accuracy: 0.800, auc_value:0.812, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 346.977(347.820)\n",
      " Epoch [172/200] train loss: 4.9E-02, test loss: 8.1E-01, accuracy: 0.815, auc_value:0.814, precision: 0.537, recall: 0.725, fscore: 0.617 , time: 347.222(347.816)\n",
      " Epoch [173/200] train loss: 3.1E-02, test loss: 8.1E-01, accuracy: 0.815, auc_value:0.814, precision: 0.537, recall: 0.725, fscore: 0.617 , time: 348.143(347.818)\n",
      " Epoch [174/200] train loss: 3.1E-02, test loss: 8.1E-01, accuracy: 0.805, auc_value:0.815, precision: 0.517, recall: 0.750, fscore: 0.612 , time: 347.937(347.819)\n",
      " Epoch [175/200] train loss: 4.2E-02, test loss: 8.3E-01, accuracy: 0.795, auc_value:0.810, precision: 0.500, recall: 0.750, fscore: 0.600 , time: 347.466(347.817)\n",
      " Epoch [176/200] train loss: 3.1E-02, test loss: 8.3E-01, accuracy: 0.754, auc_value:0.811, precision: 0.444, recall: 0.800, fscore: 0.571 , time: 347.455(347.815)\n",
      " Epoch [177/200] train loss: 3.0E-02, test loss: 8.3E-01, accuracy: 0.800, auc_value:0.813, precision: 0.509, recall: 0.725, fscore: 0.598 , time: 347.005(347.810)\n",
      " Epoch [178/200] train loss: 3.2E-02, test loss: 8.4E-01, accuracy: 0.790, auc_value:0.810, precision: 0.492, recall: 0.750, fscore: 0.594 , time: 346.469(347.803)\n",
      " Epoch [179/200] train loss: 2.7E-02, test loss: 8.5E-01, accuracy: 0.795, auc_value:0.810, precision: 0.500, recall: 0.750, fscore: 0.600 , time: 348.046(347.804)\n",
      " Epoch [180/200] train loss: 4.2E-02, test loss: 8.6E-01, accuracy: 0.805, auc_value:0.806, precision: 0.518, recall: 0.725, fscore: 0.604 , time: 345.660(347.792)\n",
      " Epoch [181/200] train loss: 3.3E-02, test loss: 8.7E-01, accuracy: 0.790, auc_value:0.808, precision: 0.492, recall: 0.750, fscore: 0.594 , time: 346.172(347.783)\n",
      " Epoch [182/200] train loss: 3.0E-02, test loss: 8.5E-01, accuracy: 0.805, auc_value:0.809, precision: 0.518, recall: 0.725, fscore: 0.604 , time: 348.330(347.786)\n",
      " Epoch [183/200] train loss: 3.5E-02, test loss: 8.4E-01, accuracy: 0.805, auc_value:0.809, precision: 0.517, recall: 0.750, fscore: 0.612 , time: 347.583(347.785)\n",
      " Epoch [184/200] train loss: 2.7E-02, test loss: 8.4E-01, accuracy: 0.800, auc_value:0.809, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 348.400(347.788)\n",
      " Epoch [185/200] train loss: 3.1E-02, test loss: 8.3E-01, accuracy: 0.800, auc_value:0.809, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 346.778(347.783)\n",
      " Epoch [186/200] train loss: 3.1E-02, test loss: 8.4E-01, accuracy: 0.805, auc_value:0.810, precision: 0.517, recall: 0.750, fscore: 0.612 , time: 347.599(347.782)\n",
      " Epoch [187/200] train loss: 3.3E-02, test loss: 8.3E-01, accuracy: 0.800, auc_value:0.810, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 347.360(347.780)\n",
      " Epoch [188/200] train loss: 2.9E-02, test loss: 8.3E-01, accuracy: 0.800, auc_value:0.810, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 346.931(347.775)\n",
      " Epoch [189/200] train loss: 2.8E-02, test loss: 8.4E-01, accuracy: 0.800, auc_value:0.809, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 345.393(347.763)\n",
      " Epoch [190/200] train loss: 2.7E-02, test loss: 8.4E-01, accuracy: 0.800, auc_value:0.809, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 346.156(347.754)\n",
      " Epoch [191/200] train loss: 2.9E-02, test loss: 8.4E-01, accuracy: 0.800, auc_value:0.809, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 346.425(347.747)\n",
      " Epoch [192/200] train loss: 2.8E-02, test loss: 8.4E-01, accuracy: 0.800, auc_value:0.810, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 356.937(347.795)\n",
      " Epoch [193/200] train loss: 2.8E-02, test loss: 8.4E-01, accuracy: 0.800, auc_value:0.810, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 362.390(347.871)\n",
      " Epoch [194/200] train loss: 3.0E-02, test loss: 8.5E-01, accuracy: 0.800, auc_value:0.809, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 359.216(347.929)\n",
      " Epoch [195/200] train loss: 3.4E-02, test loss: 8.4E-01, accuracy: 0.800, auc_value:0.810, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 347.436(347.927)\n",
      " Epoch [196/200] train loss: 2.8E-02, test loss: 8.4E-01, accuracy: 0.800, auc_value:0.810, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 348.459(347.929)\n",
      " Epoch [197/200] train loss: 2.9E-02, test loss: 8.4E-01, accuracy: 0.800, auc_value:0.810, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 346.621(347.923)\n",
      " Epoch [198/200] train loss: 3.3E-02, test loss: 8.4E-01, accuracy: 0.800, auc_value:0.810, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 346.239(347.914)\n",
      " Epoch [199/200] train loss: 3.0E-02, test loss: 8.4E-01, accuracy: 0.800, auc_value:0.810, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 357.741(347.964)\n",
      " Epoch [200/200] train loss: 3.0E-02, test loss: 8.4E-01, accuracy: 0.800, auc_value:0.810, precision: 0.508, recall: 0.750, fscore: 0.606 , time: 343.089(347.939)\n",
      "Cross validation accuracy mean: 0.886, std 0.032 \n",
      "Cross validation auc mean: 0.922, std 0.025 \n",
      "Cross validation precision mean: 0.675, std 0.080 \n",
      "Cross validation recall mean: 0.874, std 0.062 \n",
      "Cross validation fscore mean: 0.759, std 0.061 \n"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "seed_torch(seed)\n",
    "\n",
    "p, l = get_patient_label(r\"F:\\lung_dl\\data\\subtyping\\TCGA-BRCA R50\\label.csv\")#########################################################################################\n",
    "index = [i for i in range(len(p))]\n",
    "random.shuffle(index)\n",
    "p = p[index]\n",
    "l = l[index]\n",
    "\n",
    "if cv_fold > 1:\n",
    "    train_p, train_l, test_p, test_l,val_p,val_l = get_kflod(cv_fold, p, l,val_ratio)\n",
    "\n",
    "acs, pre, rec,fs,auc,te_auc,te_fs=[],[],[],[],[],[],[]\n",
    "ckc_metric = [acs, pre, rec,fs,auc,te_auc,te_fs]\n",
    "\n",
    "\n",
    "for k in range(fold_start, cv_fold):\n",
    "    print('Start %d-fold cross validation: fold %d ' % (cv_fold, k))\n",
    "    ckc_metric = one_fold(k,ckc_metric,train_p, train_l, test_p, test_l,val_p,val_l)\n",
    "\n",
    "print('Cross validation accuracy mean: %.3f, std %.3f ' % (np.mean(np.array(acs)), np.std(np.array(acs))))\n",
    "print('Cross validation auc mean: %.3f, std %.3f ' % (np.mean(np.array(auc)), np.std(np.array(auc))))\n",
    "print('Cross validation precision mean: %.3f, std %.3f ' % (np.mean(np.array(pre)), np.std(np.array(pre))))\n",
    "print('Cross validation recall mean: %.3f, std %.3f ' % (np.mean(np.array(rec)), np.std(np.array(rec))))\n",
    "print('Cross validation fscore mean: %.3f, std %.3f ' % (np.mean(np.array(fs)), np.std(np.array(fs))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
