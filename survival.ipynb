{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import csv\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=7):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9d73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCGA_Survival(data.Dataset):\n",
    "    def __init__(self, excel_file):\n",
    "        print('[dataset] loading dataset from %s' % (excel_file))\n",
    "        rows = pd.read_csv(excel_file)\n",
    "        self.rows = self.disc_label(rows)\n",
    "        label_dist = self.rows['Label'].value_counts().sort_index()\n",
    "        print('[dataset] discrete label distribution: ')\n",
    "        print(label_dist)\n",
    "        print('[dataset] dataset from %s, number of cases=%d' % (excel_file, len(self.rows)))\n",
    "    def get_split(self, fold=0):\n",
    "        random.seed(1)\n",
    "        ratio=0.2\n",
    "        assert 0 <= fold <= 4, 'fold should be in 0 ~ 4'\n",
    "        sample_index = random.sample(range(len(self.rows)), len(self.rows)) \n",
    "        num_split = round((len(self.rows) - 1) * ratio)\n",
    "        if fold < 1 / ratio - 1: \n",
    "            val_split = sample_index[fold * num_split: (fold + 1) * num_split]\n",
    "        else:\n",
    "            val_split = sample_index[fold * num_split:]\n",
    "        train_split = [i for i in sample_index if i not in val_split]\n",
    "        print(\"[dataset] training split: {}, validation split: {}\".format(len(train_split), len(val_split)))\n",
    "        return train_split, val_split\n",
    "    def __getitem__(self, index):\n",
    "        case = self.rows.iloc[index, :].values.tolist()\n",
    "        ID, Event, Status, WSI = case[:4]\n",
    "        Label = case[-1]\n",
    "        Censorship = 1 if int(Status) == 0 else 0\n",
    "        fo=r\"F:\\lung_dl\\data\\survival\\TCGA-LUSC-R50\\pt\"\n",
    "        WSI = torch.load(f\"{fo}/{ID}.pt\")\n",
    "        return (ID, WSI, Event, Censorship, Label)\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "    def disc_label(self, rows):\n",
    "        n_bins, eps = 4, 1e-6\n",
    "        uncensored_df = rows[rows['Status'] == 1] \n",
    "        disc_labels, q_bins = pd.qcut(uncensored_df['Event'], q=n_bins, retbins=True, labels=False) \n",
    "        q_bins[-1] = rows['Event'].max() + eps\n",
    "        q_bins[0] = rows['Event'].min() - eps\n",
    "        disc_labels, q_bins = pd.cut(rows['Event'], bins=q_bins, retbins=True, labels=False, right=False, include_lowest=True) \n",
    "        disc_labels = disc_labels.values.astype(int)\n",
    "        disc_labels[disc_labels < 0] = -1\n",
    "        rows.insert(len(rows.columns), 'Label', disc_labels)\n",
    "        return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "results_dir = \"./results/{dataset}/[{model}]-[{time}]\".format(\n",
    "    dataset=\"TCGA-LUSC-R50\",##########################################################################################\n",
    "    model=\"topomil\",\n",
    "    time=time.strftime(\"%Y-%m-%d]-[%H-%M-%S\"),\n",
    ")\n",
    "print(results_dir)\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5383370",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "n_features = 1024 # plip:512\\resnet50:1024############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TCGA_Survival(excel_file=r\"F:\\lung_dl\\data\\survival\\TCGA-LUSC-PLIP\\LUSC.csv\")######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc999d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CV_Meter():\n",
    "    def __init__(self, fold=5):\n",
    "        self.fold = fold\n",
    "        self.header = [\"folds\", \"fold 0\", \"fold 1\", \"fold 2\", \"fold 3\", \"fold 4\", \"mean\", \"std\"]\n",
    "        self.epochs = [\"epoch\"]\n",
    "        self.cindex = [\"cindex\"]\n",
    "\n",
    "    def updata(self, score, epoch):\n",
    "        self.epochs.append(epoch)\n",
    "        self.cindex.append(round(score, 4))\n",
    "\n",
    "    def save(self, path):\n",
    "        self.cindex.append(round(np.mean(self.cindex[1:self.fold + 1]), 4))\n",
    "        self.cindex.append(round(np.std(self.cindex[1:self.fold + 1]), 4))\n",
    "        print(\"save evaluation resluts to\", path)\n",
    "        with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as fp:\n",
    "            writer = csv.writer(fp)\n",
    "            writer.writerow(self.header)\n",
    "            writer.writerow(self.epochs)\n",
    "            writer.writerow(self.cindex)\n",
    "meter = CV_Meter(fold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663fe5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "def initialize_weights(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            # ref from huggingface\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m,nn.Linear):\n",
    "            # ref from clam\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m,nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, GlobalAttention\n",
    "\n",
    "class TopoAggregator(nn.Module):\n",
    "    def __init__(self, dim_in=512, dim_hidden=512, topk=6):\n",
    "        super().__init__()\n",
    "        self.proj_q = nn.Linear(dim_in, dim_hidden)\n",
    "        self.proj_k = nn.Linear(dim_in, dim_hidden)\n",
    "        self.topk = topk\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.proj_q(x)  # Query\n",
    "        k = self.proj_k(x) \n",
    "\n",
    "        S = torch.matmul(q, k.transpose(-2, -1))  \n",
    "        S_topk, idx_topk = torch.topk(S, k=self.topk, dim=-1)\n",
    "        idx_topk = idx_topk.to(torch.long)\n",
    "\n",
    "        idx_topk_exp = idx_topk.expand(k.size(0), -1, -1)\n",
    "        batch_indices = torch.arange(k.size(0)).view(-1, 1, 1).to(idx_topk.device)\n",
    "        K_neighbors = k[batch_indices, idx_topk_exp, :]\n",
    "\n",
    "        P_topk = F.softmax(S_topk, dim=2)\n",
    "        X_agg = torch.mul(P_topk.unsqueeze(-1), K_neighbors) + torch.matmul((1 - P_topk).unsqueeze(-1), q.unsqueeze(2))\n",
    "\n",
    "        G = torch.tanh(X_agg)\n",
    "        W_KA = torch.einsum('ijkl,ijkm->ijk', K_neighbors, G)\n",
    "        P_KA = F.softmax(W_KA, dim=2).unsqueeze(2)\n",
    "        X_topo = torch.matmul(P_KA, K_neighbors).squeeze(2)\n",
    "\n",
    "        return X_topo + q\n",
    "\n",
    "\n",
    "class DAttention(nn.Module):\n",
    "    def __init__(self,input_dim,n_classes,TopoAggregator=None):\n",
    "        super(DAttention, self).__init__()\n",
    "        self.L = 512 #512\n",
    "        self.D = 128 #128\n",
    "        self.K = 1\n",
    "        self.feature = [nn.Linear(input_dim, 512)]# nn.LayerNorm(input_dim),\n",
    "        self.feature += [nn.ReLU()]# \n",
    "\n",
    "        #if dropout:\n",
    "        self.feature += [nn.Dropout(0.25)]\n",
    "        if TopoAggregator is not None:\n",
    "            self.feature += [TopoAggregator] \n",
    "        self.feature = nn.Sequential(*self.feature)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            # nn.LayerNorm(self.L),\n",
    "            nn.Linear(self.L, self.D),\n",
    "            # nn.LayerNorm(self.D),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.D, self.K)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.L*self.K, n_classes),\n",
    "        )\n",
    "\n",
    "        self.apply(initialize_weights)\n",
    "    def forward(self, x):\n",
    "\n",
    "        feature = self.feature(x) #1 * N * 512\n",
    "        feature = feature.squeeze(0) # N * 512 \n",
    "        A = self.attention(feature) # N * 1\n",
    "\n",
    "        A = torch.transpose(A, -1, -2)  # 1*N\n",
    "        A = F.softmax(A, dim=-1)  # 1 * N \n",
    "\n",
    "        M = torch.mm(A, feature)  # KxL\n",
    "        Y_prob = self.classifier(M)\n",
    "        hazards = torch.sigmoid(Y_prob)\n",
    "        S = torch.cumprod(1 - hazards, dim=1)\n",
    "        return hazards, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5175a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def define_loss(args):\n",
    "    if args == \"ce_surv\":\n",
    "        loss = CrossEntropySurvLoss(alpha=0.0)\n",
    "    elif args == \"nll_surv\":\n",
    "        loss = NLLSurvLoss(alpha=0.0)\n",
    "    elif args == \"nll_surv_l1\":\n",
    "        loss = [NLLSurvLoss(alpha=0.0), nn.L1Loss()]\n",
    "    elif args == \"nll_surv_mse\":\n",
    "        loss = [NLLSurvLoss(alpha=0.0), nn.MSELoss()]\n",
    "    elif args == \"nll_surv_kl\":\n",
    "        loss = [NLLSurvLoss(alpha=0.0), KLLoss()]\n",
    "    elif args == \"nll_surv_cos\":\n",
    "        loss = [NLLSurvLoss(alpha=0.0), CosineLoss()]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return loss\n",
    "\n",
    "\n",
    "def nll_loss(hazards, S, Y, c, alpha=0.4, eps=1e-7):\n",
    "    batch_size = len(Y)\n",
    "    Y = Y.view(batch_size, 1)  # ground truth bin, 1,2,...,k\n",
    "    c = c.view(batch_size, 1).float()  # censorship status, 0 or 1\n",
    "    if S is None:\n",
    "        S = torch.cumprod(1 - hazards, dim=1)  # surival is cumulative product of 1 - hazards\n",
    "    # without padding, S(0) = S[0], h(0) = h[0]\n",
    "    S_padded = torch.cat([torch.ones_like(c), S], 1)  # S(-1) = 0, all patients are alive from (-inf, 0) by definition\n",
    "    # after padding, S(0) = S[1], S(1) = S[2], etc, h(0) = h[0]\n",
    "    # h[y] = h(1)\n",
    "    # S[1] = S(1)\n",
    "    uncensored_loss = -(1 - c) * (\n",
    "        torch.log(torch.gather(S_padded, 1, Y).clamp(min=eps)) + torch.log(torch.gather(hazards, 1, Y).clamp(min=eps))\n",
    "    )\n",
    "    censored_loss = -c * torch.log(torch.gather(S_padded, 1, Y + 1).clamp(min=eps))\n",
    "    neg_l = censored_loss + uncensored_loss\n",
    "    loss = (1 - alpha) * neg_l + alpha * uncensored_loss\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def ce_loss(hazards, S, Y, c, alpha=0.4, eps=1e-7):\n",
    "    batch_size = len(Y)\n",
    "    Y = Y.view(batch_size, 1)  # ground truth bin, 1,2,...,k\n",
    "    c = c.view(batch_size, 1).float()  # censorship status, 0 or 1\n",
    "    if S is None:\n",
    "        S = torch.cumprod(1 - hazards, dim=1)  # surival is cumulative product of 1 - hazards\n",
    "    # without padding, S(0) = S[0], h(0) = h[0]\n",
    "    # after padding, S(0) = S[1], S(1) = S[2], etc, h(0) = h[0]\n",
    "    # h[y] = h(1)\n",
    "    # S[1] = S(1)\n",
    "    S_padded = torch.cat([torch.ones_like(c), S], 1)\n",
    "    reg = -(1 - c) * (torch.log(torch.gather(S_padded, 1, Y) + eps) + torch.log(torch.gather(hazards, 1, Y).clamp(min=eps)))\n",
    "    ce_l = -c * torch.log(torch.gather(S, 1, Y).clamp(min=eps)) - (1 - c) * torch.log(1 - torch.gather(S, 1, Y).clamp(min=eps))\n",
    "    loss = (1 - alpha) * ce_l + alpha * reg\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "class CrossEntropySurvLoss(object):\n",
    "    def __init__(self, alpha=0.15):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, hazards, S, Y, c, alpha=None):\n",
    "        if alpha is None:\n",
    "            return ce_loss(hazards, S, Y, c, alpha=self.alpha)\n",
    "        else:\n",
    "            return ce_loss(hazards, S, Y, c, alpha=alpha)\n",
    "\n",
    "\n",
    "# loss_fn(hazards=hazards, S=S, Y=Y_hat, c=c, alpha=0)\n",
    "class NLLSurvLoss(object):\n",
    "    def __init__(self, alpha=0.15):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, hazards, S, Y, c, alpha=None):\n",
    "        if alpha is None:\n",
    "            return nll_loss(hazards, S, Y, c, alpha=self.alpha)\n",
    "        else:\n",
    "            return nll_loss(hazards, S, Y, c, alpha=alpha)\n",
    "\n",
    "\n",
    "class KLLoss(object):\n",
    "    def __call__(self, y, y_hat):\n",
    "        return F.kl_div(y_hat.softmax(dim=-1).log(), y.softmax(dim=-1), reduction=\"sum\")\n",
    "\n",
    "\n",
    "class CosineLoss(object):\n",
    "    def __call__(self, y, y_hat):\n",
    "        return 1 - F.cosine_similarity(y, y_hat, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "def define_optimizer(args, model,lr=0.001,weight_decay=0.0001):##############################################################################\n",
    "    if args == 'SGD':\n",
    "        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    elif args == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "    elif args == 'Adam':\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "    elif args == 'RAdam':\n",
    "        optimizer = RAdam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "    elif args == 'PlainRAdam':\n",
    "        optimizer = PlainRAdam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "    elif args == 'Lookahead':\n",
    "        base_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "        optimizer = Lookahead(base_optimizer)\n",
    "    else:\n",
    "        raise NotImplementedError('Optimizer [{}] is not implemented'.format(args))\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "    '''\n",
    "    RAdam Optimizer  \n",
    "    Implementation lifted from: https://github.com/LiyuanLucasLiu/RAdam\n",
    "    Paper: `On the Variance of the Adaptive Learning Rate and Beyond` - https://arxiv.org/abs/1908.03265\n",
    "    '''\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = group['lr'] * math.sqrt(\n",
    "                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
    "                                N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0 and group['weight_decay'] is not None:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class PlainRAdam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "\n",
    "        super(PlainRAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(PlainRAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                beta2_t = beta2 ** state['step']\n",
    "                N_sma_max = 2 / (1 - beta2) - 1\n",
    "                N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    step_size = group['lr'] * math.sqrt(\n",
    "                        (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
    "                            N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    '''\n",
    "    Lookahead Optimizer Wrapper\n",
    "    Implementation modified from: https://github.com/alphadl/lookahead.pytorch\n",
    "    Paper: `Lookahead Optimizer: k steps forward, 1 step back` - https://arxiv.org/abs/1907.08610\n",
    "    Hacked together by / Copyright 2020 Ross Wightman\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base_optimizer, alpha=0.5, k=6):\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults = base_optimizer.defaults\n",
    "        self.defaults.update(defaults)\n",
    "        self.state = defaultdict(dict)\n",
    "        # manually add our defaults to the param groups\n",
    "        for name, default in defaults.items():\n",
    "            for group in self.param_groups:\n",
    "                group.setdefault(name, default)\n",
    "\n",
    "    def update_slow(self, group):\n",
    "        for fast_p in group[\"params\"]:\n",
    "            if fast_p.grad is None:\n",
    "                continue\n",
    "            param_state = self.state[fast_p]\n",
    "            if 'slow_buffer' not in param_state:\n",
    "                param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n",
    "                param_state['slow_buffer'].copy_(fast_p.data)\n",
    "            slow = param_state['slow_buffer']\n",
    "            slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n",
    "            fast_p.data.copy_(slow)\n",
    "\n",
    "    def sync_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update_slow(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        # assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n",
    "        loss = self.base_optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            group['lookahead_step'] += 1\n",
    "            if group['lookahead_step'] % group['lookahead_k'] == 0:\n",
    "                self.update_slow(group)\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.base_optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict['state']\n",
    "        param_groups = fast_state_dict['param_groups']\n",
    "        return {\n",
    "            'state': fast_state,\n",
    "            'slow_state': slow_state,\n",
    "            'param_groups': param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        fast_state_dict = {\n",
    "            'state': state_dict['state'],\n",
    "            'param_groups': state_dict['param_groups'],\n",
    "        }\n",
    "        self.base_optimizer.load_state_dict(fast_state_dict)\n",
    "\n",
    "        # We want to restore the slow state, but share param_groups reference\n",
    "        # with base_optimizer. This is a bit redundant but least code\n",
    "        slow_state_new = False\n",
    "        if 'slow_state' not in state_dict:\n",
    "            print('Loading state_dict from optimizer without Lookahead applied.')\n",
    "            state_dict['slow_state'] = defaultdict(dict)\n",
    "            slow_state_new = True\n",
    "        slow_state_dict = {\n",
    "            'state': state_dict['slow_state'],\n",
    "            'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n",
    "        if slow_state_new:\n",
    "            # reapply defaults to catch missing lookahead specific ones\n",
    "            for name, default in self.defaults.items():\n",
    "                for group in self.param_groups:\n",
    "                    group.setdefault(name, default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794640d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "\n",
    "def define_scheduler(args, optimizer,num_epoch=200):#################################################\n",
    "    if args == 'exp':\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, 0.1, last_epoch=-1)\n",
    "    elif args == 'step':\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=num_epoch / 2, gamma=0.1)\n",
    "    elif args == 'plateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
    "    elif args == 'cosine':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epoch, eta_min=0)\n",
    "    elif args == 'None':\n",
    "        scheduler = None\n",
    "    else:\n",
    "        return NotImplementedError('Scheduler [{}] is not implemented'.format(args))\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d4d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import torch.optim\n",
    "class Engine(object):\n",
    "    def __init__(self, results_dir, fold):\n",
    "        self.results_dir = results_dir\n",
    "        self.fold = fold\n",
    "        self.best_scores = 0\n",
    "        self.best_epoch = 0\n",
    "        self.filename_best = None\n",
    "\n",
    "    def learning(self, model, train_loader, val_loader, criterion, optimizer, scheduler,resume,evaluate,num_epoch):\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "        if resume is not None:\n",
    "            if os.path.isfile(resume):\n",
    "                print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "                checkpoint = torch.load(resume)\n",
    "                self.best_scores = checkpoint['best_score']\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "                print(\"=> loaded checkpoint (score: {})\".format(checkpoint['best_score']))\n",
    "            else:\n",
    "                print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "\n",
    "        if evaluate:\n",
    "            self.validate(val_loader, model, criterion)\n",
    "            return\n",
    "\n",
    "        for epoch in range(num_epoch):\n",
    "            self.epoch = epoch\n",
    "            # train for one epoch\n",
    "            self.train(train_loader, model, criterion, optimizer)\n",
    "            # evaluate on validation set\n",
    "            scores = self.validate(val_loader, model, criterion)\n",
    "            # remember best c-index and save checkpoint\n",
    "            is_best = scores > self.best_scores\n",
    "            if is_best:\n",
    "                self.best_scores = scores\n",
    "                self.best_epoch = self.epoch\n",
    "                self.save_checkpoint({\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best_score': self.best_scores})\n",
    "            print(' *** best score={:.4f} at epoch {}'.format(self.best_scores, self.best_epoch))\n",
    "            scheduler.step()\n",
    "            print('>>>')\n",
    "            print('>>>')\n",
    "        return self.best_scores, self.best_epoch\n",
    "\n",
    "    def train(self, data_loader, model, criterion, optimizer):\n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        all_risk_scores = np.zeros((len(data_loader)))\n",
    "        all_censorships = np.zeros((len(data_loader)))\n",
    "        all_event_times = np.zeros((len(data_loader)))\n",
    "        dataloader = tqdm(data_loader, desc='Train Epoch {}'.format(self.epoch))\n",
    "        for batch_idx, (data_ID, data_WSI, data_Event, data_Censorship, data_Label) in enumerate(dataloader):\n",
    "            if torch.cuda.is_available():\n",
    "                data_WSI = data_WSI.cuda()\n",
    "                data_Label = data_Label.type(torch.LongTensor).cuda()\n",
    "                data_Censorship = data_Censorship.type(torch.FloatTensor).cuda()\n",
    "            # prediction\n",
    "            \n",
    "            hazards, S = model(data_WSI)\n",
    "            loss = criterion(hazards=hazards, S=S, Y=data_Label, c=data_Censorship)\n",
    "            # results\n",
    "            risk = -torch.sum(S, dim=1).detach().cpu().numpy()\n",
    "            all_risk_scores[batch_idx] = risk\n",
    "            all_censorships[batch_idx] = data_Censorship.item()\n",
    "            all_event_times[batch_idx] = data_Event\n",
    "            total_loss += loss.item()\n",
    "            # backward to update parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # calculate loss and error for each epoch\n",
    "        loss = total_loss / len(dataloader)\n",
    "        c_index = concordance_index_censored((1 - all_censorships).astype(bool), all_event_times, all_risk_scores, tied_tol=1e-08)[0]\n",
    "        print('loss: {:.4f}, c_index: {:.4f}'.format(loss, c_index))\n",
    "\n",
    "\n",
    "    def validate(self, data_loader, model, criterion):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        all_risk_scores = np.zeros((len(data_loader)))\n",
    "        all_censorships = np.zeros((len(data_loader)))\n",
    "        all_event_times = np.zeros((len(data_loader)))\n",
    "        dataloader = tqdm(data_loader, desc='Test Epoch {}'.format(self.epoch))\n",
    "\n",
    "        for batch_idx, (data_ID, data_WSI, data_Event, data_Censorship, data_Label) in enumerate(dataloader):\n",
    "            if torch.cuda.is_available():\n",
    "                data_WSI = data_WSI.cuda()\n",
    "                data_Label = data_Label.type(torch.LongTensor).cuda()\n",
    "                data_Censorship = data_Censorship.type(torch.FloatTensor).cuda()\n",
    "            # prediction\n",
    "            with torch.no_grad():\n",
    "                hazards, S = model(data_WSI)\n",
    "            loss = criterion(hazards=hazards, S=S, Y=data_Label, c=data_Censorship)\n",
    "            total_loss += loss.item()\n",
    "            # results\n",
    "            risk = -torch.sum(S, dim=1).detach().cpu().numpy()\n",
    "            all_risk_scores[batch_idx] = risk\n",
    "            all_censorships[batch_idx] = data_Censorship.item()\n",
    "            all_event_times[batch_idx] = data_Event\n",
    "        # calculate loss and error for each epoch\n",
    "        loss = total_loss / len(dataloader)\n",
    "        c_index = concordance_index_censored((1 - all_censorships).astype(bool), all_event_times, all_risk_scores, tied_tol=1e-08)[0]\n",
    "        print('loss: {:.4f}, c_index: {:.4f}'.format(loss, c_index))\n",
    "        return c_index\n",
    "\n",
    "    def save_checkpoint(self, state):\n",
    "        if self.filename_best is not None:\n",
    "            os.remove(self.filename_best)\n",
    "        fold_dir = os.path.join(self.results_dir, f'fold_{self.fold}')\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        self.filename_best = os.path.join(self.results_dir,\n",
    "                                          'fold_' + str(self.fold),\n",
    "                                          'model_best_{score:.4f}_{epoch}.pth.tar'.format(score=state['best_score'], epoch=state['epoch']))\n",
    "        print('save best model {filename}'.format(filename=self.filename_best))\n",
    "        torch.save(state, self.filename_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92afd865",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(5):\n",
    "    # get split\n",
    "    train_split, val_split = dataset.get_split(fold)\n",
    "    train_loader = DataLoader(dataset, batch_size=1, num_workers=0, pin_memory=True, sampler=SubsetRandomSampler(train_split))\n",
    "    val_loader = DataLoader(dataset, batch_size=1, num_workers=0, pin_memory=True, sampler=SubsetRandomSampler(val_split))\n",
    "    # build model, criterion, optimizer, schedular\n",
    "    #################################################\n",
    "    # Unimodal: WSI\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    age = TopoAggregator(dim_in=512, dim_hidden=512, topk=6).to(device) #######################################\n",
    "    model = DAttention(input_dim=1024, n_classes=4, TopoAggregator=age).to(device)################################################################################input_dim R50：1024\n",
    "    engine = Engine(results_dir, fold)\n",
    "    criterion = define_loss(\"nll_surv\")\n",
    "    optimizer = define_optimizer(\"Adam\", model)\n",
    "    scheduler = define_scheduler('cosine', optimizer)\n",
    "    # start training\n",
    "    score, epoch = engine.learning(model, train_loader, val_loader, criterion, optimizer, scheduler,resume=None,evaluate=False,num_epoch=200)\n",
    "    meter.updata(score, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(results_dir, \"results_{}.csv\".format(\"topomil\"))\n",
    "meter.save(csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
